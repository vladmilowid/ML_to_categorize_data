{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Выпускная квалификационная работа\n",
        "### на тему «Применение машинного обучения для категоризации тестовых данных»\n",
        "Выполнил студент группы ЗБ-ПИ20(2) Миловидов В.И."
      ],
      "metadata": {
        "id": "K6XNbZS8BD7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl"
      ],
      "metadata": {
        "id": "cK6zrSgvBF4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tabulate"
      ],
      "metadata": {
        "id": "F-u9E5TeBeAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "xjhVttFMBn82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "SM6ETKkdBpGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optree"
      ],
      "metadata": {
        "id": "WjwLVoi9BqJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade optree"
      ],
      "metadata": {
        "id": "QSFLtZYhBrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade umap-learn"
      ],
      "metadata": {
        "id": "TZdXQ0gzBsDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from openpyxl import load_workbook\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import timeit\n",
        "from tabulate import tabulate\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sklearn_crfsuite\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import learning_curve"
      ],
      "metadata": {
        "id": "pJ0wnehiBuK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "e_DFIBtFBybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 1: Чтение данных из xlsx файла\n",
        "workbook = load_workbook(filename='dat_min_500_61k.xlsx')\n",
        "sheet = workbook.active\n",
        "data = []\n",
        "for row in sheet.iter_rows(values_only=True):\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['text', 'product'])"
      ],
      "metadata": {
        "id": "4-iw79zFB0gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Анализ данных"
      ],
      "metadata": {
        "id": "fO7ZCKcBB1rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# выводим данные\n",
        "df"
      ],
      "metadata": {
        "id": "ChKnPwFYB2Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Вывод статистической сводки датафрейма (Выявление аномальных значений)\n",
        "table = tabulate(df.describe(), headers='keys', tablefmt='psql')\n",
        "print(table)"
      ],
      "metadata": {
        "id": "wmnbGdytB3ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Вывод шкалы измерения признака \"Продукт\"\n",
        "print(\"Уникальные значения 'product':\", df['product'].nunique())\n",
        "print(df['product'].value_counts())\n"
      ],
      "metadata": {
        "id": "OPikhgNPB5Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из информации выше можно сделать следующие выводы:\n",
        "- В колонке \"product\" имеется 28 уникальных значения.\n",
        "- Самым частым значением в колонке \"product\" является \"Оплата картой (Прямые мерчанты)\" с частотой 15430.\n",
        "- Значение \"СБП\" встречается 8066 раз.\n",
        "- Также отмечается, что значение \"SberPay\" с наименьшей частотой составляет 158.\n"
      ],
      "metadata": {
        "id": "rXpP_IR5B75u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Вывод обобщенной информации о датафрейме\n",
        "info_table = tabulate(df.info(), headers='keys', tablefmt='psql')\n",
        "print(info_table)"
      ],
      "metadata": {
        "id": "jpzWJNsEB8qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Проверка на пропущенные/пустые значения\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "3ZdNbT7uB-mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Описательный анализ"
      ],
      "metadata": {
        "id": "qlqVzkQ2CEW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Визуализация распределения признака 'product'\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Построение диаграммы\n",
        "df['product'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2E-RShFPCEvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Выявление коррелирующих признаков и признаков, не несущих информации для данной задачи\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vec = TfidfVectorizer()\n",
        "X = vec.fit_transform(df['text'])\n",
        "y = df['product']\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Посмотрим на важность каждого слова\n",
        "importances = pd.DataFrame(data=model.coef_[0], index=vec.get_feature_names_out(), columns=['importance'])\n",
        "importances.sort_values('importance', ascending=False, inplace=True)\n",
        "print(importances)\n",
        "importances.to_excel('importances.xlsx')\n",
        "print(\"Файл 'importances.xlsx' успешно сохранен\")"
      ],
      "metadata": {
        "id": "zrsS6hVHCHIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из предоставленной информации, которая содержит значения и их важность (importance), можно сделать следующие выводы:\n",
        "\n",
        "1. Высокая важность: Некоторые значения имеют высокую важность, такие как \"проверяется\" (importance: 3.427926), \"карты\" (importance: 3.141554), \"отклонения\" (importance: 2.693837), \"дс\" (importance: 2.650492) и \"кода\" (importance: 2.618930). Высокая важность этих значений может указывать на их значимость или влияние на анализ или обработку данных.\n",
        "2. Низкая важность: Некоторые значения имеют низкую важность, такие как \"сумму\" (importance: -1.051133), \"иэ\" (importance: -1.142789), \"на\" (importance: -1.164227), \"сбп\" (importance: -1.178481) и \"возврат\" (importance: -2.073418). Низкая важность может указывать на то, что эти значения имеют меньшее влияние на анализ или обработку данных.\n",
        "3. Влияние на принятие решений: при анализе полного списка значения с высокими и низкими показателями важности имели большое влияние на принятие решени. Значения с выскоим importance - встали в основу \"ключевых слов\" при преварительной обработке текста. Значения с низким importance - заложили основу стоп-слов в иденичном процессе\n"
      ],
      "metadata": {
        "id": "fM9n9xhTCLbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Word Cloud (Облако слов) - визуализация\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Объединяем все текстовые сообщения в одну строку\n",
        "text = ' '.join(df['text'])\n",
        "\n",
        "# Создаем Word Cloud и отображаем его\n",
        "wordcloud = WordCloud(width=800, height=400).generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eaQbc1RvCMQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heatmap (Тепловая карта)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Создаем тепловую карту\n",
        "heatmap_data = df.groupby(['text', 'product']).size().unstack(fill_value=0)\n",
        "sns.heatmap(heatmap_data, cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KEh2V8nWCQB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод: при текущий значениях - метод не релевантен для анализа\n",
        "\n",
        "## Предообработка данных\n",
        "В этом шаге проведем предварительную обработку текста, чтобы подготовить его к дальнейшему применению. Мы будем использовать некоторые основные методы обработки текста, включая удаление стоп-слов и лемматизацию слов.\n",
        "\n",
        "1. Сначала мы инициализируем список стоп-слов на русском языке с помощью stop_words = list(stopwords.words('russian'))\n",
        "2. Затем дополняем список стоп-слов некоторыми дополнительными фразами и словами, которые хотим исключить из текста с помощью stop_words.extend([...])\n",
        "3. Затем вводим функцию preprocess_text(text), которая принимает текст в качестве входного аргумента. Внутри этой функции выполняем следующие действия:\n",
        "    - Преобразуем текст в нижний регистр с помощью text.lower().\n",
        "    - Разбиваем текст на отдельные слова (токены) с использованием word_tokenize().\n",
        "    - Производим лемматизацию каждого токена с помощью объекта lemmatizer.lemmatize(token).\n",
        "    - Оставляем только буквенно-цифровые токены с помощью if token.isalnum().\n",
        "    - Исключаем стоп-слова из списка токенов с помощью if token not in stop_words.\n",
        "    - Добавляем ключевые слова к списку токенов, чтобы учитывать их при анализе.\n",
        "    - Формируем окончательную строку путем объединения токенов с помощью ' '.join(tokens).\n",
        "4. Применяем данную функцию к столбцу \"text\" в DataFrame df и сохраняем результат в столбце \"Processed_Text\"\n"
      ],
      "metadata": {
        "id": "UQis6yCOCVD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 2: Предварительная обработка текста\n",
        "stop_words = list(stopwords.words('russian'))\n",
        "\n",
        "stop_words.extend([\n",
        " 'С уважением',\n",
        " 'Группа МПС',\n",
        " 'МПС',\n",
        " 'Отдел сопровождения',\n",
        " 'ПЦ',\n",
        " 'Контакты',\n",
        " 'Вернемся с ответом в понедельник до по МСК',\n",
        " 'Вернемся с ответом',\n",
        " 'МСК',\n",
        " 'до',\n",
        " 'по',\n",
        " 'Мы зарегистрировали ваше обращение и возьмем его в работу в ближайшее время',\n",
        " 'Номер заявки',\n",
        " 'Ответы на популярные вопросы можно найти в Тинькофф Помощи',\n",
        " 'Команда поддержки Тинькофф Касса',\n",
        " 'Январь',\n",
        " 'Февраль',\n",
        " 'Март',\n",
        " 'Апрель',\n",
        " 'Май',\n",
        " 'Июнь',\n",
        " 'Июль',\n",
        " 'Август',\n",
        " 'Сентябрь',\n",
        " 'Октябрь',\n",
        " 'Ноябрь',\n",
        " 'Декабрь',\n",
        " 'ФИО',\n",
        " 'Используй кейс',\n",
        " 'Сразу добавь в копию почту почты из столбца справа если указано',\n",
        " 'Комментарий',\n",
        " 'Обращение создано через формы на',\n",
        " 'Просьба',\n",
        " 'Менеджер по развитию партнеров',\n",
        " 'Отдела продаж',\n",
        " 'Департамента платежных систем',\n",
        " 'Наталия Алексуткина',\n",
        " 'Моб',\n",
        " 'Доп',\n",
        " 'Это электронное письмо строго конфиденциально включая все вложения',\n",
        " 'Ссылка на чат с клиентом',\n",
        " 'Отсутствует',\n",
        " 'Власов Александр',\n",
        " 'Технический специалист',\n",
        " 'Мы в и',\n",
        " 'Информация содержащаяся в данном письме и приложенных к нему материалах составляет коммерческую тайну и является конфиденциальной в связи с чем воспроизведение копирование распространение или использование информации любым иным образом может осуществляться исключительно с согласия уполномоченного представителя',\n",
        " 'Если данное письмо было получено вами ошибочно просим незамедлительно удалить письмо',\n",
        " 'пожалуйста',\n",
        " 'нажимайте на ответить всем не видно ваших ответов',\n",
        " 'Менеджер по развитию бизнеса',\n",
        " 'Менеджер по развитию',\n",
        " 'Команда сопровождения платежей и переводов',\n",
        " 'спасибо',\n",
        " 'Заранее',\n",
        " 'посмотрите',\n",
        " 'запрос',\n",
        " 'Уточните',\n",
        " 'Отдел сверок',\n",
        " 'Ссылка на чат с клиентом',\n",
        " 'Возобновление задачи',\n",
        " 'Можем',\n",
        " 'Мы',\n",
        " 'Руководитель проектов',\n",
        " 'Департамент платежных систем',\n",
        " 'Ростелеком',\n",
        " 'Вопрос в работе',\n",
        " 'Вернемся с ответом до',\n",
        " 'Прошу',\n",
        " 'В чем может быть проблема',\n",
        " 'Поддержка продуктов для бизнеса',\n",
        " 'Бэк офис',\n",
        " 'скриншот',\n",
        " 'прикрепляю',\n",
        " 'Сообщите',\n",
        " 'номер тикета',\n",
        " 'Специалист по работе с ключевыми клиентами',\n",
        " 'необходимо',\n",
        " 'подскажите',\n",
        " 'умеем',\n",
        " 'помогите',\n",
        " 'поступило обращение',\n",
        " 'поддержка',\n",
        " 'Полиса',\n",
        " 'Дата и время события',\n",
        " 'Подробное описание',\n",
        " 'Пользуйся кейсами со страницы продукта',\n",
        " 'Подробности',\n",
        " 'Проблема',\n",
        " 'Отправлено из Почты',\n",
        " 'Исходное сообщение',\n",
        " 'поставьте нам оценку по ссылке',\n",
        " 'уточнить',\n",
        " 'Группа контроля эквайринга',\n",
        " 'Данное сообщение отправлено из',\n",
        " 'В случае отсутствия доп вопросов к',\n",
        " 'не отвечать на него',\n",
        " 'а так же удалить рассылку из дальнейшей переписки',\n",
        " 'можете',\n",
        " 'подсказать',\n",
        " 'с вашей стороны',\n",
        " 'Старший специалист',\n",
        " 'вопрос',\n",
        " 'Инженер группы поддержки платежей',\n",
        " 'Отдел сопровождения платежей и переводов',\n",
        " 'вы',\n",
        " 'оценить мою работу здесь',\n",
        " 'оставить чаевые здесь',\n",
        " 'Менеджер по работе с партнерами',\n",
        " 'прощение',\n",
        " 'прощения',\n",
        " 'пишу',\n",
        " 'Сектор поддержки бизнеса',\n",
        " 'Отдел претензий',\n",
        " 'Управление',\n",
        " 'Департамент',\n",
        " 'Департамент клиентского обслуживания',\n",
        " 'для звонков из России',\n",
        " 'для звонков из-за границы',\n",
        " 'Факс',\n",
        " 'из-за',\n",
        " 'из за',\n",
        " 'чат',\n",
        " 'Я ваш клиент',\n",
        " 'Заранее благодарю',\n",
        " 'Приносим извинения за доставленные неудобства',\n",
        " 'извинения',\n",
        " 'Диалог',\n",
        " 'Персональный менеджер',\n",
        " 'Направление корпоративного бизнеса',\n",
        " 'Моб',\n",
        " 'Бизнес аналитик',\n",
        " 'аналитик',\n",
        " 'добрый день',\n",
        " 'здравствуйте',\n",
        " 'добрый',\n",
        " 'привет',\n",
        " 'коллеги',\n",
        " 'тел',\n",
        " 'доб',\n",
        " 'доброе утро',\n",
        " 'доброе',\n",
        " 'добрый вечер',\n",
        " 'колеги'])\n",
        "\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    start_time = timeit.default_timer()\n",
        "    if text is not None:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Добавление ключевых слов\n",
        "    keywords = [\n",
        " 'qr',\n",
        " 'sbp',\n",
        " 'notifications',\n",
        " 'notification',\n",
        " 'cloudkassir',\n",
        " 'rrn',\n",
        " 'api',\n",
        " 'iss_serno',\n",
        " 'sdk',\n",
        " 'cms',\n",
        " 'tinkoff-pay',\n",
        " 'tinkoffpay',\n",
        " 'mirpay',\n",
        " 'tinkoff pay',\n",
        " 'returns amount',\n",
        " 'msm',\n",
        " 'acq',\n",
        " 'acqapi',\n",
        " 'compliance',\n",
        " 'ovd',\n",
        " 'tilda',\n",
        " 'bitrix',\n",
        " 'orangedata',\n",
        " 'orange data',\n",
        " 'ofd ferma',\n",
        " 'ferma',\n",
        " 'ecomkassa',\n",
        " 'advantshop',\n",
        " 'amiro',\n",
        " 'amocrm',\n",
        " 'cs-cart',\n",
        " 'diafan',\n",
        " 'drupal commerce',\n",
        " 'drupal ubercart',\n",
        " 'ecshop',\n",
        " 'ecwid',\n",
        " 'getcourse',\n",
        " 'joomla hikashop',\n",
        " 'hostcms',\n",
        " 'image cms',\n",
        " 'insales',\n",
        " 'joomla',\n",
        " 'joomshopping',\n",
        " 'virtuemart',\n",
        " 'magento',\n",
        " 'maxystore',\n",
        " 'modx shopkeeper',\n",
        " 'mogutacms',\n",
        " 'netcat',\n",
        " 'opencart',\n",
        " 'oscommerce',\n",
        " 'phpshop.cms',\n",
        " 'prestashop',\n",
        " 'quickbuy',\n",
        " 'simpla',\n",
        " 'taplink',\n",
        " 'umi.cms',\n",
        " 'vamshop',\n",
        " 'vigbo',\n",
        " 'webasyst',\n",
        " 'wfolio',\n",
        " 'wordpress ecommerce',\n",
        " 'wordpress shop',\n",
        " 'wordpress',\n",
        " 'woocommerce',\n",
        " 'seller',\n",
        " 'chargeback',\n",
        " 'mapi',\n",
        " 'arn',\n",
        " 'ssl',\n",
        " 'mcc',\n",
        " 'bnpl',\n",
        " 'outward',\n",
        " 'outwards',\n",
        " 'atol',\n",
        " 'webhook',\n",
        " 'wl',\n",
        " 'white list',\n",
        " 'whitelist',\n",
        " 'dolyame']\n",
        "\n",
        "    tokens += keywords\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['Processed_Text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_7K8XnzGCWUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Применение машинного обучения"
      ],
      "metadata": {
        "id": "ApxifFxDCdyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 3: Разделение данных на обучающий и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Processed_Text'], df['product'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "aYay28xiCeid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритмы для классификации текста, которые будут проанализированы далее:\n",
        "- Логистическая регрессия\n",
        "- Метод k ближайших соседей (k-Nearest Neighbors, k-NN)\n",
        "- Метод наивного Байеса (Naive Bayes)\n",
        "- Методы опорных векторов (Support Vector Machines, SVM)\n",
        "- Решающие деревья и случайный лес\n",
        "- Градиентный бустинг (Gradient Boosting)\n",
        "- Нейронные сети\n",
        "- Метод максимальной энтропии (Maximum Entropy)\n",
        "- Методы гибридной классификации"
      ],
      "metadata": {
        "id": "M5xQpHNbChJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Логистическая регрессия (LogisticRegression)"
      ],
      "metadata": {
        "id": "pHN3YPOtCjOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели логистической регрессии\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели\n",
        "y_pred_lr = classifier.predict(X_test_vect)\n",
        "print(\"Отчет модели логистической регресии:\")\n",
        "print(classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "OnGU59ehClQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Производительность модели по классам: Модель показывает высокие значения F1-меры для некоторых классов, таких как \"Онлайн-касса\", \"Реестры, акты и выгрузки\", \"Рассрочка на платежной форме\", \"Оплата картой (Прямые мерчанты)\". Это свидетельствует о хорошей способности модели распознавать и классифицировать тексты, относящиеся к этим классам.\n",
        "2. Развитие точности и полноты: Значения точности (precision) и полноты (recall) для различных классов могут отличаться. Например, для класса \"Оплата картой (Прямые мерчанты)\" значение точности составляет 0.53, в то время как значение полноты составляет 0.83.\n",
        "    - Precision (Точность): Precision представляет собой долю правильно классифицированных положительных образцов относительно всех образцов, которые модель классифицировала как положительные. Судя по результатам, модель продемонстрировала высокую точность (precision) для некоторых классов, таких как \"Tinkoff Pay Wallet\" (0.98), \"Tinkoff ID\" (0.98), \"Рассрочка на платежной форме\" (0.99). Это говорит о том, что модель имеет небольшую вероятность ложной классификации для этих классов.\n",
        "    - Recall (Полнота): Recall отражает способность модели правильно идентифицировать положительные образцы относительно всех фактически положительных образцов в наборе данных. Некоторые классы, такие как \"Онлайн-касса\" (0.82), \"Оплата картой (Прямые мерчанты)\" (0.83), \"Рассрочка на платежной форме\" (0.91), показывают высокую полноту. Это означает, что модель не пропускает много положительных образцов, связанных с этими классами.\n",
        "    - F1-score: F1-score является средневзвешенной метрикой, объединяющей точность и полноту, которая учитывает баланс между этими двумя метриками. В целом, средневзвешенное значение F1-меры составляет около 0.68, что говорит о сбалансированных результатов модели по всем классам. Высокие значения F1-score наблюдаются для классов с высокой точностью и полнотой. Некоторые классы с высоким F1-score включают \"Рассрочка на платежной форме\" (0.95), \"Онлайн-касса\" (0.75), \"Реестры, акты и выгрузки\" (0.80). Однако некоторые классы, такие как \"3DS\", \"Маркетплейс (переводы)\", \"Вне компетенции поддержки ИЭ\", имеют очень низкие значения F1-score (меньше 0.10). Это может указывать на трудности в классификации этих конкретных классов.\n",
        "4. Улучшение производительности: Чтобы улучшить производительность модели, можно рассмотреть следующие подходы: Использование более сложной модели или композиции моделей, которые могут лучше учитывать особенности текстовых данных.\n",
        "\n",
        "\n",
        "Вывод: Результаты статистики свидетельствуют о разной производительности модели для разных классов. Некоторые классы демонстрируют высокую точность, полноту и F1-score, что говорит о хорошей способности модели правильно классифицировать эти классы. Однако, некоторые классы имеют низкие значения этих метрик, что может указывать на сложности и недостаток данных для этих классов. Для улучшения производительности модели с учетом особенностей и требований нашей задачи, рассмортим следующий алгоритм\n"
      ],
      "metadata": {
        "id": "jKe-TSAPCqk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод k ближайших соседей (k-Nearest Neighbors, k-NN)"
      ],
      "metadata": {
        "id": "tLO1IRpjCsSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели k-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели\n",
        "y_pred_knn = knn.predict(X_test_vect)\n",
        "print(\"Отчет модели k ближайших соседей:\")\n",
        "print(classification_report(y_test, y_pred_knn))"
      ],
      "metadata": {
        "id": "bQNvHwZ2CuBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Точность (precision): Модели имеют разную точность для разных классов. Некоторые классы имеют высокую точность (например, \"Маркетплейс (переводы)\", \"Модули CMS и виджеты\", \"Онлайн-касса\"), в то время как другие классы имеют низкую точность (например, \"3DS\", \"СБП E2C (Выплаты)\", \"Вне компетенции поддержки ИЭ\"). Средняя точность по всем классам составляет около 0.54, что означает, что в среднем модели правильно классифицируют около 54% примеров.\n",
        "2. Полнота (recall): Подобно точности, модели имеют разную полноту для разных классов. Некоторые классы имеют высокую полноту (например, \"Тинькофф Pay Wallet\", \"Реестры, акты и выгрузки\", \"СБП\"), в то время как другие классы имеют низкую полноту (например, \"3DS\", \"Маркетплейс (переводы)\", \"Вне компетенции поддержки ИЭ\"). Средняя полнота по всем классам составляет около 0.44, что означает, что в среднем модели правильно распознают около 44% примеров.\n",
        "3. F1-мера (f1-score): F1-мера является сбалансированной метрикой, учитывающей и точность, и полноту. Поэтому она может дать представление о совокупной производительности модели. Средняя F1-мера по всем классам составляет около 0.47, что означает, что в среднем модели достигают F1-меры около 47%.\n",
        "\n",
        "Вывод: сравнивая два метода Логистическая регрессия, где средний F1-score = 69% и Метод k ближайших соседей (k-Nearest Neighbors, k-NN), где средний F1-score = 59% - можем сделать вывод, что Логическая регрессия определяет классы с более точным показателем (будем сравнивать его с последющему алгоритмами)\n"
      ],
      "metadata": {
        "id": "q8TSLqAXCx1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод наивного Байеса (Naive Bayes)"
      ],
      "metadata": {
        "id": "iXqyYRZDCzB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели Метода наивного Байеса\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели\n",
        "y_pred_nb = naive_bayes.predict(X_test_vect)\n",
        "print(\"Отчет модели наивного Байеса:\")\n",
        "print(classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "id": "xXEcj9TRC0aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Точность (precision): Большинство классов имеют низкую точность, равную нулю или близкую к нулю. Ограниченное количество классов имеют высокую точность, близкую к 1.0. Взвешенное среднее значение точности составляет около 0.60, что указывает на относительно низкую точность модели в целом.\n",
        "2. Полнота (recall): Идентичная ситуация, как и с точностью. Взвешенное среднее значение полноты составляет около 0.41, что указывает на относительно низкую полноту модели в целом.\n",
        "3. F1-мера (f1-score): Среднее взвешенное значение F1-меры составляет около 0.34, что указывает на относительно низкую сбалансированную производительность модели.\n",
        "\n",
        "Таким образом, анализируя предоставленную статистику, можно сделать вывод, что модели имеют проблемы с предсказанием многих классов, показывая низкую точность, полноту и F1-меру. Некоторые классы могут быть обнаружены с высокой точностью, но здесь таких классов явно мало.\n"
      ],
      "metadata": {
        "id": "ku2ORvTiC11x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Методы опорных векторов (Support Vector Machines, SVM)"
      ],
      "metadata": {
        "id": "wGVZlWOAC3nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели Методов опорных векторов (SVM)\n",
        "svm = SVC()\n",
        "svm.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели\n",
        "y_pred_svm = svm.predict(X_test_vect)\n",
        "print(\"Отчет модели опорных векторов:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "KCHQ3oLNC5SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Точность (precision): Некоторые классы имеют высокую точность (например, \"3DS\", \"BNPL (Долями)\", \"E-inv\", \"SberPay\", \"Tinkoff ID\", \"Tinkoff Pay Wallet\", \"Безопасная сделка\", \"Выплаты (A2C)\", \"Кастомизация ПФ\", \"Онлайн-касса\", \"СБП\", \"СБП E2C (Выплаты)\"), превышающую 0.8. Некоторые классы имеют низкую точность (например, \"Вне компетенции поддержки ИЭ\", \"Маркетплейс (переводы)\"), близкую к нулю или ниже 0.5. Средняя точность по всем классам составляет около 0.74, что указывает на хорошую общую точность модели.\n",
        "2. Полнота (recall): Некоторые классы имеют высокую полноту (например, \"Origination\", \"Tinkoff Pay Wallet\", \"Модули CMS и виджеты\", \"Онлайн-касса\", \"СБП\"), превышающую 0.7. Некоторые классы имеют низкую полноту (например, \"Вне компетенции поддержки ИЭ\", \"Маркетплейс (переводы)\"), близкую к нулю или ниже 0.3. Средняя полнота по всем классам составляет около 0.70, что указывает на среднюю общую полноту модели.\n",
        "3. F1-мера (f1-score): Многие классы имеют высокие значения F1-меры, которые близки к 1.0 (например, \"Origination\", \"Tinkoff Pay Wallet\", \"Модули CMS и виджеты\", \"Онлайн-касса\", \"СБП\"). Некоторые классы имеют низкие значения F1-меры, близкие к нулю или ниже 0.3 (например, \"Вне компетенции поддержки ИЭ\", \"Маркетплейс (переводы)\"). Средняя F1-мера по всем классам составляет около 0.70, что указывает на хорошую сбалансированную производительность модели.\n",
        "\n",
        "Таким образом, метод опорных векторов (Support Vector Machines, SVM) превзошел преыдущие алгоритмы. Сравним в наилучшим показатлем по Логистической регрессии, где f1-score = 69%, против текущий 70%. План-base, что был поставлен в рамках требований к проекту, успешно выполнен. Следующая итерация - добиться максимального улучшения предикшина путем перебора альтернативных методов\n"
      ],
      "metadata": {
        "id": "PU8bvObsC6ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решающих деревьев (DecisionTreeClassifier)\n"
      ],
      "metadata": {
        "id": "A4j_BmybC770"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели Решающих деревьев\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели Решающих деревьев\n",
        "y_pred_decision_tree = decision_tree.predict(X_test_vect)\n",
        "print(\"Отчет модели Решающих деревьев:\")\n",
        "print(classification_report(y_test, y_pred_decision_tree))"
      ],
      "metadata": {
        "id": "wnsg3VnfC-Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "1. Процент правильно классифицированных образцов (accuracy) составляет 59%, что означает, что модель способна правильно предсказывать класс в примерно 59% случаев.\n",
        "2. Метрика precision (точность), измеряющая долю верно предсказанных положительных классов среди всех положительных предсказаний, имеет среднее значение 49%\n",
        "3. Метрика recall (полнота), измеряющая способность модели верно классифицировать все действительные положительные классы, имеет среднее значение 45%\n",
        "4. Метрика f1-score (F1-мера), которая является гармоническим средним между точностью и полнотой, имеет среднее значение 59%\n",
        "\n",
        "Вывод: модель имеет средние результаты в предсказании классов, с переменными результатами в зависимости от конкретного класса.\n"
      ],
      "metadata": {
        "id": "0LnGS-zTDDKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Случайный лес (RandomForestClassifier)"
      ],
      "metadata": {
        "id": "YbKG8qZoDDxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели Случайного леса\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели Случайного леса\n",
        "y_pred_random_forest = random_forest.predict(X_test_vect)\n",
        "print(\"Отчет модели Случайного леса:\")\n",
        "print(classification_report(y_test, y_pred_random_forest))\n"
      ],
      "metadata": {
        "id": "D3Ydy_4pDHbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Recall (полнота) модели Случайного леса различается в зависимости от класса. Некоторые классы имеют низкое значение полноты, что означает, что модель определяет только малую часть реально положительных случаев для этих классов.\n",
        "2. F1-мера, объединяющая точность и полноту, также показывает смешанные результаты для различных классов. Некоторые классы имеют низкую F1-меру, что указывает на недостаточно хорошее баланс между точностью и полнотой. Общая точность (accuracy) модели составляет 59%, что означает, что около 59% образцов были правильно классифицированы моделью.\n",
        "\n",
        "Вывод: модель Случайного леса имеет смешанные результаты производительности для разных классов.\n"
      ],
      "metadata": {
        "id": "qKvGdIJzDGO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Градиентный бустинг (Gradient Boosting)"
      ],
      "metadata": {
        "id": "rzuV1KCKDKOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 3: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 4: Обучение модели градиентного бустинга\n",
        "gradient_boosting = GradientBoostingClassifier()\n",
        "gradient_boosting.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 5: Оценка модели градиентного бустинга\n",
        "y_pred_gradient_boosting = gradient_boosting.predict(X_test_vect)\n",
        "print(\"Отчет модели градиентного бустинга:\")\n",
        "print(classification_report(y_test, y_pred_gradient_boosting))\n"
      ],
      "metadata": {
        "id": "wZHJtRyODMHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "- Было сделано 12387 предсказаний с использованием модели градиентного бустинга.\n",
        "- Общая точность модели составляет 0.64, что означает, что около 64% предсказаний были классифицированы правильно.\n",
        "- Для большинства категорий наибольшие значения точности, полноты и F1-меры наблюдаются для категорий, таких как \"Tinkoff Pay Wallet\" (0.93), \"Рассрочка на платежной форме\" (0.93) и \"Онлайн-касса\" (0.79), что указывает на хорошую эффективность модели при предсказании этих категорий.\n",
        "- С другой стороны, некоторые категории имеют низкие значения точности, полноты и F1-меры. Например, категории \"3DS\", \"СБП E2C (Выплаты)\" и \"Маркетплейс (переводы)\" имеют очень низкую точность и полноту, что указывает на сложности с классификацией этих категорий для модели градиентного бустинга.\n",
        "\n",
        "В целом, средние значения точности, полноты и F1-меры (macro avg) составляют 0.47, что означает, что модель имеет средние результаты в общем смысле.\n",
        "Средневзвешенная точность, полнота и F1-мера (weighted avg) также составляют 0.64, что отражает средние значения метрик с учетом дисбаланса классов.\n",
        "В общем и целом, модель градиентного бустинга показывает средние результаты в предсказании различных категорий. Из отчета видно, что некоторые категории успешно классифицируются моделью, в то время как у других категорий есть проблемы с точностью и полнотой классификации.\n"
      ],
      "metadata": {
        "id": "KePbs2UdDNS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Нейронные сети\n"
      ],
      "metadata": {
        "id": "jZpwEbiVDOgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 3: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 4: Обучение модели нейронных сетей\n",
        "mlp = MLPClassifier()\n",
        "mlp.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 5: Оценка модели нейронных сетей\n",
        "y_pred_mlp = mlp.predict(X_test_vect)\n",
        "print(\"Отчет модели нейронных сетей:\")\n",
        "print(classification_report(y_test, y_pred_mlp))\n"
      ],
      "metadata": {
        "id": "yispA02RDP0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "- Общая точность модели составляет 0.64\n",
        "- Также, как алгоритм Градиентный бустинг (Gradient Boosting), метод нейронных сетей демонстрируют высокие значения точности, полноты и F1-меры: Tinkoff Pay Wallet (0.94), Рассрочка на платежной форме (0.95) и Онлайн-касса (0.72) имеют высокие показатели.Также, категории СБП (0.76) и Реестры, акты и выгрузки (0.75) имеют хорошие результаты.\n",
        "- Некоторые категории демонстрируют низкие значения точности, полноты и F1-меры: 3DS (0.16), SDK (0.38) и Маркетплейс (переводы) (0.12) имеют низкие показатели. Вне компетенции поддержки ИЭ (0.09) также имеет низкую точность и полноту.\n",
        "- Модель показывает средние значения метрик, как macro avg, так и weighted avg, соответственно 0.54 и 0.63.\n",
        "\n",
        "Исходя из этих результатов, можно сделать вывод, что модель нейронных сетей имеет средние показатели в предсказании различных категорий.\n"
      ],
      "metadata": {
        "id": "kX_-PbakDS2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод максимальной энтропии (Maximum Entropy)"
      ],
      "metadata": {
        "id": "1EtpB3IdDUUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 3: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 4: Обучение модели метода максимальной энтропии\n",
        "max_ent = LogisticRegression()\n",
        "max_ent.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 5: Оценка модели метода максимальной энтропии\n",
        "y_pred_max_ent = max_ent.predict(X_test_vect)\n",
        "print(\"Отчет модели метода максимальной энтропии:\")\n",
        "print(classification_report(y_test, y_pred_max_ent))\n"
      ],
      "metadata": {
        "id": "vSIWzrr5DWaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать следующие выводы:\n",
        "\n",
        "1. Точность (precision): Некоторые классы имеют высокую точность (например, \"BNPL (Долями)\", \"Origination\", \"Tinkoff ID\", \"Tinkoff Pay Wallet\", \"Безопасная сделка\", \"Выплаты (A2C)\", \"Онлайн-касса\", \"Реестры, акты и выгрузки\"), превышающую 0.75. Средняя точность по всем классам составляет около 0.71, что указывает на хорошую общую точность модели.\n",
        "2. Полнота (recall): Некоторые классы имеют высокую полноту (например, \"Origination\", \"Tinkoff Pay Wallet\", \"Модули CMS и виджеты\", \"Онлайн-касса\", \"Реестры, акты и выгрузки\"), превышающую 0.7. Средняя полнота по всем классам составляет около 0.69, что указывает на неплохую общую полноту модели.\n",
        "3. F1-мера (f1-score): Средняя F1-мера по всем классам составляет около 0.68, что указывает на хорошую сбалансированную производительность модели.\n",
        "\n",
        "Вывод: Метод максимальной энтропии (Maximum Entropy) является релевантным и стоит рядом с Методы опорных векторов (Support Vector Machines, SVM) и Логистическая регрессия (LogisticRegression)\n"
      ],
      "metadata": {
        "id": "FNR0hPUFDeXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Методы гибридной классификации"
      ],
      "metadata": {
        "id": "sHxk9jc3Df_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 4: Создание пайплайна для преобразования и классификации\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', SVC(kernel='linear'))\n",
        "])\n",
        "\n",
        "# Шаг 5: Обучение и оценка модели с помощью пайплайна\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_cross = pipeline.predict(X_test)\n",
        "print(\"Отчет модели:\")\n",
        "print(classification_report(y_test, y_pred_cross))\n"
      ],
      "metadata": {
        "id": "hM5e1D6JDhFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "- Общая точность модели составляет 0.69, что означает, что около 69% предсказаний были классифицированы правильно.\n",
        "- Модель показывает средние значения метрик, как macro avg, так и weighted avg, соответственно 0.56 и 0.68.\n",
        "- Метод гибридной классификации является одним из лидеров при анализе десяти разный способов алгоритмизации обучения модели. Дале будем рассматривать его более детально с применением наилучших показателей гиперпараметров\n",
        "\n"
      ],
      "metadata": {
        "id": "_HZcAR8cDioa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Улучшение модели"
      ],
      "metadata": {
        "id": "yfss-4vZDjrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После анализа текущих показателей было выдвинуто несколько гипотез\n",
        "- удаление некоторых тем из исходной датасета. Цель увеличить качечество датасета (избавить от шума) и повысить процент предсказания от моделей\n",
        "- улучшить модель в качетсве предобработки данных. При анализе видно, что классы с худшими показателями, явлются те классы, что имеют минимальное кол-во представленных текстовых примеров. Таким образом, дабы сбалансировать весь датасет, было принято решенеие использовать алгоритм балансировки классов Применение методов балансировки классов позволяет улучшить производительность модели, сделав ее более справедливой при работе с несбалансированными наборами данных. Существуют два основных подхода к балансировке классов: oversampling (увеличение примеров минорного класса) и undersampling (сокращение примеров мажорного класса).\n",
        "\n",
        "При первой итерации можо выделать следующие аолгоритмы, которые мы будем тестировать дальше:\n",
        "- Методы опорных векторов (Support Vector Machines, SVM), где общее accuracy f1-score  = 0,69 (отработало за 52 мин.)\n",
        "- Методы гибридной классификации, где общее accuracy f1-score  = 0,69 (отаработало за 27 мин.)\n",
        "- Логистическая регрессия (LogisticRegression), где общее accuracy f1-score  = 0,67 (отработало за 2 мин.)\n",
        "- Метод максимальной энтропии (Maximum Entropy), где общее accuracy f1-score  = 0,67 (отработало за 1 мин.)\n",
        "\n",
        "\n",
        "Гипотезы будем проверять параллельно друг другу.\n",
        "Во-первых, выделил отлеьно файл, где удалил неакутальные темы: \"Маркетплейс (переводы)\", \"3DS\", \"SberPay\", \"E-inv\", \"Tinkoff Pay Wallet\", \"Нерезиденты\". Актуальный файл - dat_min_500_menshe_topicov.\n",
        "Во-вторых, проанализировав темы по количетсву, где бОльшая часть классов является минорными по отношению к классам, как Оплата картой (прямые мерчанты) около 15к и CБП около 8к, было принято решение использовать метод oversampling для балансировки классов.\n",
        "Oversampling (увеличение примеров минорного класса) - это процесс увеличения примеров минорного класса путем генерации новых записей на основе имеющихся примеров.\n"
      ],
      "metadata": {
        "id": "kXP8EdUCDmPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Балансировка классов"
      ],
      "metadata": {
        "id": "U0XOHNBGDsNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer(lowercase=True)\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Балансировка классов с помощью oversampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Обучение модели логистической регрессии на увеличенных данных\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Шаг 7: Оценка модели\n",
        "y_pred = classifier.predict(X_test_vect)\n",
        "print(\"Отчет модели логистической регрессии:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Pcg34dCiDuxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 1: Чтение данных из переработанного xlsx файла\n",
        "workbook = load_workbook(filename='dat_min_500_menshe_topicov.xlsx')\n",
        "sheet = workbook.active\n",
        "data = []\n",
        "for row in sheet.iter_rows(values_only=True):\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['text', 'product'])\n"
      ],
      "metadata": {
        "id": "sw0DNohYDwHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Вывод шкалы измерения признака \"Продукт\"\n",
        "print(\"Уникальные значения 'product':\", df['product'].nunique())\n",
        "print(df['product'].value_counts())\n"
      ],
      "metadata": {
        "id": "IE4DuSMlDxIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предобработка данных\n"
      ],
      "metadata": {
        "id": "bUkzpHGBDyCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 2: Предварительная обработка текста\n",
        "stop_words = list(stopwords.words('russian'))\n",
        "\n",
        "stop_words.extend([\n",
        " 'С уважением',\n",
        " 'Группа МПС',\n",
        " 'МПС',\n",
        " 'Отдел сопровождения',\n",
        " 'ПЦ',\n",
        " 'Контакты',\n",
        " 'Вернемся с ответом в понедельник до по МСК',\n",
        " 'Вернемся с ответом',\n",
        " 'МСК',\n",
        " 'до',\n",
        " 'по',\n",
        " 'Мы зарегистрировали ваше обращение и возьмем его в работу в ближайшее время',\n",
        " 'Номер заявки',\n",
        " 'Ответы на популярные вопросы можно найти в Тинькофф Помощи',\n",
        " 'Команда поддержки Тинькофф Касса',\n",
        " 'Январь',\n",
        " 'Февраль',\n",
        " 'Март',\n",
        " 'Апрель',\n",
        " 'Май',\n",
        " 'Июнь',\n",
        " 'Июль',\n",
        " 'Август',\n",
        " 'Сентябрь',\n",
        " 'Октябрь',\n",
        " 'Ноябрь',\n",
        " 'Декабрь',\n",
        " 'ФИО',\n",
        " 'Используй кейс',\n",
        " 'Сразу добавь в копию почту почты из столбца справа если указано',\n",
        " 'Комментарий',\n",
        " 'Обращение создано через формы на',\n",
        " 'Просьба',\n",
        " 'Менеджер по развитию партнеров',\n",
        " 'Отдела продаж',\n",
        " 'Департамента платежных систем',\n",
        " 'Наталия Алексуткина',\n",
        " 'Моб',\n",
        " 'Доп',\n",
        " 'Это электронное письмо строго конфиденциально включая все вложения',\n",
        " 'Ссылка на чат с клиентом',\n",
        " 'Отсутствует',\n",
        " 'Власов Александр',\n",
        " 'Технический специалист',\n",
        " 'Мы в и',\n",
        " 'Информация содержащаяся в данном письме и приложенных к нему материалах составляет коммерческую тайну и является конфиденциальной в связи с чем воспроизведение копирование распространение или использование информации любым иным образом может осуществляться исключительно с согласия уполномоченного представителя',\n",
        " 'Если данное письмо было получено вами ошибочно просим незамедлительно удалить письмо',\n",
        " 'пожалуйста',\n",
        " 'нажимайте на ответить всем не видно ваших ответов',\n",
        " 'Менеджер по развитию бизнеса',\n",
        " 'Менеджер по развитию',\n",
        " 'Команда сопровождения платежей и переводов',\n",
        " 'спасибо',\n",
        " 'Заранее',\n",
        " 'посмотрите',\n",
        " 'запрос',\n",
        " 'Уточните',\n",
        " 'Отдел сверок',\n",
        " 'Ссылка на чат с клиентом',\n",
        " 'Возобновление задачи',\n",
        " 'Можем',\n",
        " 'Мы',\n",
        " 'Руководитель проектов',\n",
        " 'Департамент платежных систем',\n",
        " 'Ростелеком',\n",
        " 'Вопрос в работе',\n",
        " 'Вернемся с ответом до',\n",
        " 'Прошу',\n",
        " 'В чем может быть проблема',\n",
        " 'Поддержка продуктов для бизнеса',\n",
        " 'Бэк офис',\n",
        " 'скриншот',\n",
        " 'прикрепляю',\n",
        " 'Сообщите',\n",
        " 'номер тикета',\n",
        " 'Специалист по работе с ключевыми клиентами',\n",
        " 'необходимо',\n",
        " 'подскажите',\n",
        " 'умеем',\n",
        " 'помогите',\n",
        " 'поступило обращение',\n",
        " 'поддержка',\n",
        " 'Полиса',\n",
        " 'Дата и время события',\n",
        " 'Подробное описание',\n",
        " 'Пользуйся кейсами со страницы продукта',\n",
        " 'Подробности',\n",
        " 'Проблема',\n",
        " 'Отправлено из Почты',\n",
        " 'Исходное сообщение',\n",
        " 'поставьте нам оценку по ссылке',\n",
        " 'уточнить',\n",
        " 'Группа контроля эквайринга',\n",
        " 'Данное сообщение отправлено из',\n",
        " 'В случае отсутствия доп вопросов к',\n",
        " 'не отвечать на него',\n",
        " 'а так же удалить рассылку из дальнейшей переписки',\n",
        " 'можете',\n",
        " 'подсказать',\n",
        " 'с вашей стороны',\n",
        " 'Старший специалист',\n",
        " 'вопрос',\n",
        " 'Инженер группы поддержки платежей',\n",
        " 'Отдел сопровождения платежей и переводов',\n",
        " 'вы',\n",
        " 'оценить мою работу здесь',\n",
        " 'оставить чаевые здесь',\n",
        " 'Менеджер по работе с партнерами',\n",
        " 'прощение',\n",
        " 'прощения',\n",
        " 'пишу',\n",
        " 'Сектор поддержки бизнеса',\n",
        " 'Отдел претензий',\n",
        " 'Управление',\n",
        " 'Департамент',\n",
        " 'Департамент клиентского обслуживания',\n",
        " 'для звонков из России',\n",
        " 'для звонков из-за границы',\n",
        " 'Факс',\n",
        " 'из-за',\n",
        " 'из за',\n",
        " 'чат',\n",
        " 'Я ваш клиент',\n",
        " 'Заранее благодарю',\n",
        " 'Приносим извинения за доставленные неудобства',\n",
        " 'извинения',\n",
        " 'Диалог',\n",
        " 'Персональный менеджер',\n",
        " 'Направление корпоративного бизнеса',\n",
        " 'Моб',\n",
        " 'Бизнес аналитик',\n",
        " 'аналитик',\n",
        " 'добрый день',\n",
        " 'здравствуйте',\n",
        " 'добрый',\n",
        " 'привет',\n",
        " 'коллеги',\n",
        " 'тел',\n",
        " 'доб',\n",
        " 'доброе утро',\n",
        " 'доброе',\n",
        " 'добрый вечер',\n",
        " 'колеги'])\n",
        "\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    start_time = timeit.default_timer()\n",
        "    if text is not None:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Добавление ключевых слов\n",
        "    keywords = [\n",
        " 'qr',\n",
        " 'sbp',\n",
        " 'notifications',\n",
        " 'notification',\n",
        " 'cloudkassir',\n",
        " 'rrn',\n",
        " 'api',\n",
        " 'iss_serno',\n",
        " 'sdk',\n",
        " 'cms',\n",
        " 'tinkoff-pay',\n",
        " 'tinkoffpay',\n",
        " 'mirpay',\n",
        " 'tinkoff pay',\n",
        " 'returns amount',\n",
        " 'msm',\n",
        " 'acq',\n",
        " 'acqapi',\n",
        " 'compliance',\n",
        " 'ovd',\n",
        " 'tilda',\n",
        " 'bitrix',\n",
        " 'orangedata',\n",
        " 'orange data',\n",
        " 'ofd ferma',\n",
        " 'ferma',\n",
        " 'ecomkassa',\n",
        " 'advantshop',\n",
        " 'amiro',\n",
        " 'amocrm',\n",
        " 'cs-cart',\n",
        " 'diafan',\n",
        " 'drupal commerce',\n",
        " 'drupal ubercart',\n",
        " 'ecshop',\n",
        " 'ecwid',\n",
        " 'getcourse',\n",
        " 'joomla hikashop',\n",
        " 'hostcms',\n",
        " 'image cms',\n",
        " 'insales',\n",
        " 'joomla',\n",
        " 'joomshopping',\n",
        " 'virtuemart',\n",
        " 'magento',\n",
        " 'maxystore',\n",
        " 'modx shopkeeper',\n",
        " 'mogutacms',\n",
        " 'netcat',\n",
        " 'opencart',\n",
        " 'oscommerce',\n",
        " 'phpshop.cms',\n",
        " 'prestashop',\n",
        " 'quickbuy',\n",
        " 'simpla',\n",
        " 'taplink',\n",
        " 'umi.cms',\n",
        " 'vamshop',\n",
        " 'vigbo',\n",
        " 'webasyst',\n",
        " 'wfolio',\n",
        " 'wordpress ecommerce',\n",
        " 'wordpress shop',\n",
        " 'wordpress',\n",
        " 'woocommerce',\n",
        " 'seller',\n",
        " 'chargeback',\n",
        " 'mapi',\n",
        " 'arn',\n",
        " 'ssl',\n",
        " 'mcc',\n",
        " 'bnpl',\n",
        " 'outward',\n",
        " 'outwards',\n",
        " 'atol',\n",
        " 'webhook',\n",
        " 'wl',\n",
        " 'white list',\n",
        " 'whitelist',\n",
        " 'dolyame']\n",
        "\n",
        "    tokens += keywords\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['Processed_Text'] = df['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "Z6i-QsmtD2pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Применение машионного обучения с гепперпараметрами\n"
      ],
      "metadata": {
        "id": "41Q2MOzZD3rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 3: Разделение данных на обучающий и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Processed_Text'], df['product'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "tkFxJoS_D4pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В текущей итерации будем рассматривать следующие алгоритмы:\n",
        "- Методы опорных векторов (Support Vector Machines, SVM)\n",
        "- Методы гибридной классификации\n",
        "- Логистическая регрессия (LogisticRegression)\n",
        "- Метод максимальной энтропии (Maximum Entropy)\n",
        "В рамках данной итерации сразу применю перекрестную проверку (cross_val_score)для достоверности измерения метрик моделей.\n",
        "\n",
        "При обучении моделей машинного обучения часто требуется оценить, насколько хорошо модель обобщает данные и способна предсказывать правильные результаты для новых данных. Однако при разделении данных на тренировочный и тестовый наборы возникает проблема влияния конкретной случайной выборки на результаты оценки модели. Если случайная выборка не репрезентативна или содержит смещенность (например, в одном наборе данных преобладает один класс), то точность модели на отложенном тестовом наборе может недостоверно отражать ее реальную производительность.\n",
        "\n",
        "Чтобы избежать этой проблемы, вместо единственного разделения данных на тренировочный и тестовый наборы, используется перекрестная проверка. Перекрестная проверка позволяет повторить обучение и оценку модели на различных разбиениях данных, чтобы получить более надежные оценки производительности модели.\n",
        "\n",
        "После выполнения перекрестной проверки выводится точность на каждой итерации и средняя точность на перекрестной проверке. Средняя точность позволяет оценить обобщающую способность модели на основе всех разбиений данных и будет более надежной мерой ее производительности на новых данных. Поэтому перекрестная проверка помогает оценить и выбрать модель с наиболее надежными результатами и определить лучшие гиперпараметры для этой модели.\n"
      ],
      "metadata": {
        "id": "gozNhdpBD83j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Методы опорных векторов (Support Vector Machines, SVM) с использованием GridSearchCV\n"
      ],
      "metadata": {
        "id": "On_Ywdg7D-ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 4: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 5: Обучение модели Методов опорных векторов (SVM) с использованием GridSearchCV\n",
        "svm = SVC()\n",
        "parameters = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
        "grid_search = GridSearchCV(svm, parameters, cv=5)\n",
        "grid_search.fit(X_train_vect, y_train)\n",
        "\n",
        "# Получение лучших параметров и лучшей оценки модели\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Лучшие параметры:\", best_params)\n",
        "print(\"Лучшая оценка модели:\", best_score)\n",
        "\n",
        "# Шаг 6: Оценка модели с лучшими параметрами\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_svm_giperparam = best_svm.predict(X_test_vect)\n",
        "print(\"Отчет модели опорных векторов с лучшими параметрами:\")\n",
        "print(classification_report(y_test, y_pred_svm_giperparam))\n"
      ],
      "metadata": {
        "id": "UfdIkuiMEAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном коде, создаю объект GridSearchCV с указанными значениями гиперпараметров для перебора (C и gamma). Затем, используя метод fit(), обучаю модель SVM с каждой комбинацией гиперпараметров. GridSearchCV автоматически выполнит перекрестную проверку (cross-validation) для каждой комбинации и выберет модель с наилучшей оценкой.\n",
        "\n",
        "После завершения GridSearchCV, мы получаем лучшие параметры (best_params) и лучшую оценку модели (best_score). Затем, используя best_params, создаю модель SVM с лучшими параметрами (best_svm).\n",
        "\n",
        "При использовании GridSearchCV было выявлено, что код выполняется большое время и тарит производительность ноутбук, в связи с этим было принято решение использовать RandomizedSearchCV вместо GridSearchCV для поиска лучших гиперпараметров. RandomizedSearchCV осуществляет случайный поиск по заданному пространству гиперпараметров. Данный метод считаю боле эффективным в нашем случае, где пространство поиска очень большое.\n"
      ],
      "metadata": {
        "id": "NFD8i2QGEBsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Методы гибридной классификации с использованием RandomizedSearchCV и перекрестной проверки\n"
      ],
      "metadata": {
        "id": "zL3LlPdLECn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 4: Создание пайплайна для преобразования и классификации\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "# Шаг 5: Определение гиперпараметров для поиска\n",
        "parameters = {'vectorizer__lowercase': [True, False], 'classifier__C': [0.1, 1, 10], 'classifier__gamma': [0.1, 1, 10]}\n",
        "\n",
        "# Шаг 6: Применение RandomizedSearchCV для поиска лучших гиперпараметров\n",
        "random_search = RandomizedSearchCV(pipeline, parameters, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Получение лучших параметров и лучшей оценки модели\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "print(\"Лучшие параметры:\", best_params)\n",
        "print(\"Лучшая оценка модели:\", best_score)\n",
        "\n",
        "# Шаг 7: Оценка модели с лучшими параметрами\n",
        "best_pipeline = random_search.best_estimator_\n",
        "y_pred_giperparam = best_pipeline.predict(X_test)\n",
        "print(\"Отчет модели с лучшими параметрами:\")\n",
        "print(classification_report(y_test, y_pred_giperparam))\n",
        "\n",
        "# Шаг 8: Применение перекрестной проверки для оценки модели\n",
        "cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Точность на каждой итерации перекрестной проверки:\", cv_scores)\n",
        "print(\"Средняя точность на перекрестной проверке:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "Tf-4obd0EEtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Логистическая регрессия (LogisticRegression) с использованием RandomizedSearchCV и перекрестной проверки\n"
      ],
      "metadata": {
        "id": "WN3_c1nbEHRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "# Шаг 4: Создание пайплайна для преобразования и классификации\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Шаг 5: Определение гиперпараметров для поиска\n",
        "parameters = {\n",
        "    'vectorizer__lowercase': [True, False],\n",
        "    'vectorizer__max_features': [1000, 5000, 10000],\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Шаг 6: Применение RandomizedSearchCV для поиска лучших гиперпараметров\n",
        "random_search = RandomizedSearchCV(pipeline, parameters, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Шаг 7: Оценка модели с лучшими параметрами\n",
        "best_params = random_search.best_params_\n",
        "print(\"Лучшие параметры:\", best_params)\n",
        "\n",
        "# Использование модели с лучшими параметрами для предсказания\n",
        "best_pipeline_lr = random_search.best_estimator_\n",
        "y_pred_lr_giperparam = best_pipeline_lr.predict(X_test)\n",
        "print(\"Отчет модели с лучшими параметрами:\")\n",
        "print(classification_report(y_test, y_pred_lr_giperparam))\n",
        "\n",
        "# Шаг 8: Применение перекрестной проверки для оценки модели\n",
        "cv_scores = cross_val_score(best_pipeline_lr, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Точность на каждой итерации перекрестной проверки:\", cv_scores)\n",
        "print(\"Средняя точность на перекрестной проверке:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "IsIF-MzWEKez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Анализируя предоставленную информацию, можно сделать следующие выводы:\n",
        "1. Лучшие параметры для модели:\n",
        "    - vectorizer__ngram_range: (1, 1)\n",
        "    - vectorizer__max_features: 5000\n",
        "    - vectorizer__lowercase: True\n",
        "    - classifier__penalty: 'l2'\n",
        "    - classifier__C: 10\n",
        "Эти параметры указывают на использование униграмм, ограничение до 5000 наиболее часто встречающихся признаков, преобразование текста в нижний регистр, использование регуляризации l2 и значения параметра C равное 10.\n",
        "2. Отчет модели с лучшими параметрами предоставляет информацию о точности, полноте, F1-мере и поддержке для каждой категории классов: в таблице отчета видны относительно высокие значения точности, полноты и F1-меры для некоторых категорий, таких как \"Рассрочка на платежной форме\" и \"СБП\". Однако, для некоторых категорий, таких как \"Личный кабинет\" и \"Маркетплейс\", значения этих метрик ниже.\n",
        "3. В целом, средние значения точности, полноты и F1-меры для всех категорий составляют около 0.72, что говорит о том, что модель имеет стабильную производительность для всех категорий классов. Общая точность модели составляет около 0.72, что означает, что примерно 72% всех предсказанных меток были правильными.\n",
        "4. Время выполнения модели составило около 20 минут и 32 секунды, что является более быстрым временным затратом по сравнению с предыдущим примером.\n",
        "\n",
        "Таким образом, модель с лучшими параметрами показывает приемлемую производительность, с некоторым дисбалансом в точности и полноте для некоторых категорий. Общая производительность модели сопоставима с прошлым примером, однако время выполнения значительно сократилось.\n"
      ],
      "metadata": {
        "id": "l8NKGgfTEM6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод максимальной энтропии (Maximum Entropy) с использованием GridSearchCV и перекрестной проверки\n"
      ],
      "metadata": {
        "id": "D6CXZ2s-EOJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "# Шаг 3: Создание векторизатора для преобразования текста в числовые признаки\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Шаг 4: Определение модели и параметров для поиска\n",
        "model = LogisticRegression()\n",
        "parameters = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "# Шаг 5: Применение GridSearchCV для поиска лучших гиперпараметров\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_vect, y_train)\n",
        "\n",
        "# Шаг 6: Оценка модели с лучшими параметрами\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Лучшие параметры:\", best_params)\n",
        "\n",
        "# Шаг 7: Использование модели с лучшими параметрами для предсказания\n",
        "best_model_en = grid_search.best_estimator_\n",
        "y_pred_max_ent_giperparam = best_model_en.predict(X_test_vect)\n",
        "\n",
        "# Шаг 8: Оценка модели\n",
        "print(\"Отчет модели с лучшими параметрами:\")\n",
        "print(classification_report(y_test, y_pred_max_ent_giperparam))\n",
        "\n",
        "# Шаг 9: Применение перекрестной проверки для оценки модели\n",
        "cv_scores = cross_val_score(best_model_en, X_train_vect, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Точность на каждой итерации перекрестной проверки:\", cv_scores)\n",
        "print(\"Средняя точность на перекрестной проверке:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "0Ty_3e4yEQfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Анализируя информацию можно сделать следующие выводы:\n",
        "1. Лучшие параметры для модели: {'C': 10, 'penalty': 'l2'}. Это означает, что модель достигла наилучшей производительности при использовании значений параметров C=10 и penalty=l2.\n",
        "2. В таблице отчета видны относительно высокие значения точности, полноты и F1-меры для некоторых категорий, таких как \"Рассрочка на платежной форме\" и \"СБП\". Однако для некоторых категорий, таких как \"Личный кабинет\" и \"Маркетплейс\", значения этих метрик ниже.\n",
        "3. В целом, средние значения точности, полноты и F1-меры для всех категорий составляют около 0.72, что говорит о том, что модель имеет стабильную производительность для всех категорий классов.\n",
        "4. Общая точность модели составляет около 0.73, что означает, что примерно 73% всех предсказанных меток были правильными.\n",
        "5. Время выполнения модели составило около 14 минут и 24 секунды, что является наименьшим временным затратом среди всех предоставленных примеров.\n",
        "\n",
        "Сравнивая эти результаты с предыдущими примерами, можно заметить, что модель с лучшими параметрами C=10 и penalty=l2 имеет примерно схожую производительность со всеми рассмотренным примерами.\n",
        "Время выполнения данной модели было самым низким среди предоставленных примеров, что свидетельствует о ее эффективности.\n"
      ],
      "metadata": {
        "id": "FGQopBgDERzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализация результатов моделирования\n"
      ],
      "metadata": {
        "id": "jCMZi7TLESuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализация результатов моделирования является важной частью анализа данных и может быть полезной для понимания и интерпретации производительности модели. Вот некоторые преимущества и цели визуализации результатов моделирования:\n"
      ],
      "metadata": {
        "id": "4R0RbSRdEWHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Матрица ошибок (Confusion Matrix)\n",
        "Визуализация матрицы ошибок: Матрица ошибок позволяет визуализировать сравнение между фактическими значениями и предсказанными значениями модели. Она отображает количество верных и неверных классификаций для каждого класса, что позволяет оценить точность модели, обнаружить классы с высокими или низкими показателями точности, а также выявить наличие систематических ошибок.\n",
        "Представление важных признаков: Визуализация важных признаков в модели может помочь в понимании, какие факторы имеют наибольшее влияние на предсказания модели. Это может быть полезно для принятия более информированных решений и понимания важности различных переменных.\n"
      ],
      "metadata": {
        "id": "BLkcID7xEW77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 9: Построение матрицы ошибок\n",
        "confusion_matrix = pd.crosstab(y_test, y_pred_max_ent_giperparam, rownames=['Actual'], colnames=['Predicted'])\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d')\n",
        "plt.title(\"Матрица ошибок\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DqGfBOpREYqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Диаграмма box-plot и violin-plot\n",
        "Данный тип диаграмм полез нам для сравнения распределения прогнозов для разных классов и могут дать представление о том, насколько хорошо модель разделяет классы.\n"
      ],
      "metadata": {
        "id": "TMELg6MGEaWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Шаг 9: Визуализация диаграмм box-plot и violin-plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([cv_scores], showmeans=True)\n",
        "plt.xticks([1], ['Cross Validation'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Ящик для оценок перекрестной проверки')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.violinplot(cv_scores, showmeans=True)\n",
        "plt.xticks([1], ['Cross Validation'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('График результатов перекрестной проверки для скрипки')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3gFHETzuEbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка на чистых данных\n"
      ],
      "metadata": {
        "id": "7lZxCrvlEeDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Шаг 6: Предсказание категории для нового сообщения\n",
        "new_message = \"Просим предоставить документы подтверждающие успешность транзакции\"\n",
        "new_message = preprocess_text(new_message)\n",
        "new_message_vec = vectorizer.transform([new_message])\n",
        "predicted_category = best_model_en.predict(new_message_vec)[0]\n",
        "print('Predicted Category:', predicted_category)\n"
      ],
      "metadata": {
        "id": "mUc1qPJaEfg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Функция для предобработки текста (проверил даже с предобработкой текста получаются одинаковые результаты)\n",
        "def preprocess_text(text):\n",
        "    # Приведение к нижнему регистру\n",
        "    #text = text.lower()\n",
        "\n",
        "    # Токенизация\n",
        "    #tokens = word_tokenize(text)\n",
        "\n",
        "    # Лемматизация текста\n",
        "    #lemmatizer = WordNetLemmatizer()\n",
        "    #lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Объединение лемматизированных токенов обратно в строку\n",
        "    #processed_text = ' '.join(lemmatized_tokens)\n",
        "    return text\n",
        "\n",
        "# Загрузка данных из эксель файла\n",
        "df = pd.read_excel('prediction_data_min_500.xlsx')\n",
        "texts = df.iloc[:, 0].tolist()\n",
        "\n",
        "# Создание списка для хранения предсказанных категорий\n",
        "predicted_categories = []\n",
        "\n",
        "\n",
        "for text in texts:\n",
        "    processed_text = preprocess_text(text)\n",
        "    text_vector = vectorizer.transform([processed_text])\n",
        "    text_vector = text_vector.astype('float32')  # Преобразование в тип float32\n",
        "    predicted_category = best_model_en.predict(text_vector)[0]\n",
        "    predicted_categories.append(predicted_category)\n",
        "\n",
        "# Добавление предсказанных категорий в третью колонку датафрейма\n",
        "df['Predicted Category'] = predicted_categories\n",
        "\n",
        "# Сохранение данных с предсказанными категориями обратно в эксель файл\n",
        "df.to_excel('prediction_data_min_500.xlsx', index=False)\n",
        "print('Данные с предсказанием сохранены в файле \"prediction_data_min_500.xlsx\"')\n"
      ],
      "metadata": {
        "id": "gcSrA8DvEi5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Загрузка данных из эксель файла\n",
        "df = pd.read_excel('prediction_data_min_500.xlsx')\n",
        "\n",
        "# Создание счетчика для каждого продукта\n",
        "product_counts = {}\n",
        "product_matching_counts = {}\n",
        "product_total_counts = {}\n",
        "\n",
        "# Инициализация счетчиков\n",
        "for product in df['product'].unique():\n",
        "    product_counts[product] = 0\n",
        "    product_matching_counts[product] = 0\n",
        "    product_total_counts[product] = 0\n",
        "\n",
        "# Подсчет количества каждого продукта и сравнение значений\n",
        "for index, row in df.iterrows():\n",
        "    product = row['product']\n",
        "    product_counts[product] += 1\n",
        "    product_total_counts[product] += 1\n",
        "\n",
        "    if row['product'] == row['Predicted Category']:  # Сравнение текста с продуктом\n",
        "        product_matching_counts[product] += 1\n",
        "\n",
        "total_matching_count = sum(product_matching_counts.values())\n",
        "total_non_matching_count = sum(product_total_counts.values()) - total_matching_count\n",
        "\n",
        "total_count = len(df)  # Общее количество значений\n",
        "\n",
        "# Функция для сортировки по проценту совпадений\n",
        "def sort_by_matching_percentage(item):\n",
        "    product = item[0]\n",
        "    product_matching_count = item[1]\n",
        "    return (product_matching_count / product_total_counts[product]) * 100, product\n",
        "\n",
        "# Вывод результатов по каждому продукту, отсортированных по проценту совпадений\n",
        "for product, product_matching_count in sorted(product_matching_counts.items(), key=sort_by_matching_percentage, reverse=True):\n",
        "    product_non_matching_count = product_counts[product] - product_matching_count\n",
        "    product_matching_percentage = (product_matching_count / product_total_counts[product]) * 100\n",
        "    print(f\"Для продукта {product}:\")\n",
        "    print(f\"- Количество совпадающих значений: {product_matching_count}\")\n",
        "    print(f\"- Количество несовпадающих значений: {product_non_matching_count}\")\n",
        "    print(f\"- Процент совпадающих значений: {product_matching_percentage:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Вывод общих результатов\n",
        "print(\"Общие результаты:\")\n",
        "print(f\"Количество совпадающих значений: {total_matching_count}\")\n",
        "print(f\"Количество несовпадающих значений: {total_non_matching_count}\")\n",
        "print(f\"Процент совпадающих значений: {(total_matching_count / total_count) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "dbmoEAIAEnKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Чтение данных из xlsx файла\n",
        "workbook = load_workbook(filename='product_matching_results_dat_min_500_prod.xlsx')\n",
        "sheet = workbook.active\n",
        "data = []\n",
        "for row in sheet.iter_rows(values_only=True):\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Продукт', 'Процент совпадающих значений', 'Количество значений'])\n"
      ],
      "metadata": {
        "id": "BKKeCEZGEq1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выводим данные\n",
        "df"
      ],
      "metadata": {
        "id": "nWp528H1EsjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}